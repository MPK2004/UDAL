
# â™»ï¸ MRF Digital Suite â€“ Operational Analytics & Reporting Platform
![Status](https://img.shields.io/badge/Status-Active-success)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31+-ff4b4b)

## ğŸ“Œ Overview

**MRF Digital Suite** is an end-to-end operational analytics system designed to ingest, validate, analyze, and report municipal waste-load data from Material Recovery Facilities (MRFs).

The system processes structured CSV data, stores it in a relational database, computes key operational metrics, and generates automated analytical reports (CSV & PDF) along with an interactive dashboard for real-time insights.

This project demonstrates practical data engineering, analytics, and automation workflows in a real-world operations setting.

---

## ğŸ¯ Key Features

### ğŸ”¹ Data Ingestion & Validation

* CSV import with schema normalization
* Append or overwrite import mode
* Validation for missing/invalid fields
* Duplicate record detection
* Error handling with skipped row tracking

### ğŸ”¹ Database Layer

* SQLite relational database
* Structured schema for waste_records
* Parameterized queries for safe data insertion
* Modular data access functions

### ğŸ”¹ KPI & Analytics Engine

Computes operational metrics including:

* Total waste processed
* Revenue potential (material-based pricing model)
* Weekly waste trends
* Material distribution breakdown
* Destination analysis
* Average daily waste
* Todayâ€™s operational summary

Revenue is dynamically calculated using configurable material rate mappings.

---

### ğŸ”¹ Automated Reporting

* CSV export for raw data reporting
* Styled PDF report generation using ReportLab
* Summary statistics tables
* Material-wise revenue breakdown
* Destination percentage analysis
* Time-range filtering support
* Dynamic report timestamps

---

### ğŸ”¹ Interactive Dashboard (Streamlit + Plotly)

* KPI metric cards
* Weekly trend visualization
* Material composition pie chart
* Revenue by material bar chart
* Destination breakdown
* Real-time dashboard refresh
* Downloadable data exports

---

## ğŸ— System Architecture

```
CSV Upload â†’ Validation & Cleaning â†’ SQLite DB â†’ Aggregation Logic â†’ 
KPI Engine â†’ Dashboard API â†’ Streamlit UI
                           â†“
                     CSV & PDF Reports
```

---

## ğŸ›  Tech Stack

* Python
* SQLite
* Pandas
* Streamlit
* Plotly
* ReportLab
* SQL (aggregation queries, filtering, grouping)
* Standard Python libraries (datetime, csv, os, uuid)

---

## ğŸ—‚ Project Structure

```
MRFDigital-Suite/
â”‚
â”œâ”€â”€ app.py                 # Streamlit frontend & routing logic
â”œâ”€â”€ dashboard.py           # Aggregation & KPI computation
â”œâ”€â”€ report_generator.py    # Data import, validation & PDF/CSV reporting
â”œâ”€â”€ mrf_data.db            # SQLite database file
â”œâ”€â”€ uploads/               # Uploaded CSV files
â”œâ”€â”€ reports/               # Generated reports
â”œâ”€â”€ templates/             # HTML templates (if applicable)
â”œâ”€â”€ static/                # Static assets
â””â”€â”€ README.md
```

---

## ğŸ“Š Example KPIs Generated

* Total Waste Processed (kg)
* Revenue Potential (â‚¹)
* Average Daily Waste
* Weekly Waste Trend (Last 30 Days)
* Top Material Category
* Main Destination
* Revenue by Material Type

---

## ğŸ“Š KPI & Analytics Logic

### ğŸ”¹ KPI Calculations

The dashboard computes operational KPIs directly from the `waste_records` table using SQL-style aggregation logic:

* **Total Waste Processed**
  Sum of `weight` across all records within the selected time range.

* **Average Daily Waste**
  Computed as the mean of daily aggregated totals over the last 30 days.

* **Todayâ€™s Loads**
  Derived from records where `date = current_date`.

* **Material Distribution**
  Aggregated using `GROUP BY material` to compute total weight per material type.

* **Destination Breakdown**
  Aggregated using `GROUP BY destination` to analyze waste routing patterns.

All KPIs are calculated dynamically based on real-time database queries.

---

### ğŸ’° Revenue Computation

Revenue is calculated using a predefined material rate mapping:

| Material | Rate (â‚¹/kg) |
| -------- | ----------- |
| PET      | 28          |
| HDPE     | 28          |
| Paper    | 11          |
| Glass    | 2           |
| MLP      | 4           |
| Rubber   | 4           |

For each material:

```
Revenue = Total Weight (kg) Ã— Rate (â‚¹/kg)
```

Material-wise revenue is computed after aggregation and then summed to derive total revenue potential.

This approach demonstrates how structured operational data can be transformed into business-impact metrics.

---

### ğŸ“ˆ Weekly Trend Calculation

Weekly trends are generated by:

1. Filtering records within the last 30 days.
2. Grouping by `date`.
3. Aggregating daily total waste.
4. Ordering results chronologically.

This allows visualization of:

* Operational consistency
* Peak processing days
* Volume fluctuations over time

The trend is recalculated dynamically for each dashboard load.

---

### ğŸ” Data Integrity Considerations

To ensure KPI accuracy:

* Duplicate records are detected before insertion.
* Invalid or malformed data rows are skipped.
* Append and overwrite modes are explicitly controlled.
* All aggregations are performed after validation.

This ensures reported metrics are reliable and reproducible.

---


## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Install Dependencies

```bash
pip install streamlit pandas plotly reportlab
```

### 2ï¸âƒ£ Run Application

```bash
streamlit run app.py
```

### 3ï¸âƒ£ Use the Dashboard

* Load sample data (100 synthetic records)
* Upload custom CSV data
* Generate reports
* Download exports

---

## ğŸ“¥ CSV Import Format

Expected fields:

```
date, vehicle_id, weight, material, destination
```

Alternate header variations are supported and normalized automatically.

---

## ğŸ“„ Report Output

Generated reports include:

* Total waste processed
* Number of loads
* Average load weight
* Top material & destination
* Material-wise revenue table
* Destination percentage distribution
* Time-stamped generation details

Reports are saved in the `/reports` directory.

---

## ğŸ” Data Integrity Features

* Duplicate detection before insert
* Validation for invalid weight values
* Missing essential field checks
* Optional append mode for safe incremental imports

---

## ğŸ§  What This Project Demonstrates

* End-to-end data pipeline thinking
* Structured relational data modeling
* SQL-style aggregation logic
* KPI computation aligned to business needs
* Data validation & integrity controls
* Automation of reporting workflows
* Interactive analytics visualization
* Clean modular code organization

---

## ğŸ“Œ Future Improvements

* Migration to PostgreSQL for production-scale deployment
* Role-based access control
* Logging system for import & report tracking
* Scheduled automated report generation
* REST API deployment option

---
